{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaeca595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook \n",
    "import config as cfg\n",
    "import eda_tools as tls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a5b085",
   "metadata": {},
   "source": [
    "load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c351b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration load\n",
    "log_path = Path(Path.cwd().parent /  r\"config/config.json\")\n",
    "if not log_path.exists(): \n",
    "    print(f\"Arquivo de configuração não encontrado !\\n{log_path}\")\n",
    "    sys.exit(1)\n",
    "log = cfg.config_log(log_path)\n",
    "cfg.load_config() \n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff961700",
   "metadata": {},
   "source": [
    "load sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b8adaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sheets dfs\n",
    "\n",
    "df_tables = pd.read_excel(cfg.eda_sheet_full_path,sheet_name='tables')\n",
    "df_fields = pd.read_excel(cfg.eda_sheet_full_path,sheet_name='fields', header=1)\n",
    "df_fields = df_fields.astype('object')\n",
    "df_fields.set_index([\"table\", \"field\"], inplace=True)\n",
    "\n",
    "# format headers\n",
    "df_tables.columns = df_tables.columns.str.strip().str.lower()\n",
    "df_fields.columns = df_fields.columns.str.strip().str.lower()\n",
    "\n",
    "# open sheet\n",
    "try:\n",
    "    wb = load_workbook(cfg.eda_sheet_full_path)\n",
    "    tables_sheet = wb[\"tables\"]\n",
    "    fields_sheet = wb[\"fields\"]\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro ao abrir planilhas\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4f443",
   "metadata": {},
   "source": [
    "collect describes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1d3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----ses_cias-----\n",
      "-----ses_ramos-----\n",
      "-----ses_seguros-----\n"
     ]
    }
   ],
   "source": [
    "# Estatisticas de contagem por tipo de conteudo do campo \n",
    "df_stats = pd.DataFrame(columns=['table','field','stat','value'])\n",
    "stats_collected = []\n",
    "for index, table in df_tables.iterrows():\n",
    "    # load_data\n",
    "    table_name = table['table']\n",
    "    print(f\"-----{table_name}-----\")\n",
    "    data_path = Path(cfg.data_file_path / table['file'])\n",
    "    df_dados = pd.read_csv(data_path,encoding=tls.encode(data_path),quotechar=None, quoting=3,keep_default_na=True,sep=cfg.csv_sep,engine='python')\n",
    "    df_dados.columns = df_dados.columns.str.strip().str.lower()\n",
    "    table_count = len(df_dados)\n",
    "\n",
    "    # Counts by content type \n",
    "    df_types = df_dados.map(tls.classify_content)\n",
    "    df_types.apply(pd.Series.value_counts)\n",
    "\n",
    "    for field in df_types.columns:\n",
    "        counts = df_types[field].value_counts(dropna=False)\n",
    "        for stat, value in counts.items():\n",
    "            stats_collected.append({\"table\": table_name, \"field\": field, \"stat\": stat, \"value\": int(value)})\n",
    "\n",
    "    # describe stats\n",
    "    df_fields_table = df_fields.reset_index()\n",
    "    df_fields_table = df_fields_table[df_fields_table['table'] == table_name]\n",
    "    for idx, fld in df_fields_table.iterrows(): \n",
    "        field_name = fld['field']\n",
    "        if fld['type'] == \"str\":\n",
    "            field_series = df_dados[field_name]             \n",
    "            field_series = field_series.astype('object')            \n",
    "        else: \n",
    "           field_series = pd.to_numeric(df_dados[field_name], errors='coerce')\n",
    "        # count \n",
    "        stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\": \"count\" , \"value\": table_count})\n",
    "        # min\n",
    "        stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\": \"min\" , \"value\": field_series.min()})\n",
    "        # max \n",
    "        stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\": 'max', \"value\": field_series.max()})\n",
    "        # mean \n",
    "        try: \n",
    "            stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\": \"mean\", \"value\": field_series.mean()})        \n",
    "        except: \n",
    "            stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\": \"mean\", \"value\": \"no number\"})\n",
    "        # std\n",
    "        try:        \n",
    "            stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"std\", \"value\": field_series.std()}) \n",
    "        except: \n",
    "            stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"std\", \"value\": \"no number\"})        \n",
    "        stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"unique\", \"value\": field_series.nunique()})\n",
    "        # top\n",
    "        try: \n",
    "            stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"top\", \"value\": field_series.mode().iloc[0]})\n",
    "        except: \n",
    "            stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"top\", \"value\": \"no value\"})\n",
    "        # freq\n",
    "        try: \n",
    "            stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"freq\", \"value\": field_series.value_counts().iloc[0]})\n",
    "        except: \n",
    "           stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"freq\", \"value\": \"no value\"})\n",
    "        # q1, q2, q3\n",
    "        try: \n",
    "            q_values = field_series.quantile([0.25, 0.5, 0.75])\n",
    "            stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"Q1(25%)\", \"value\": q_values.iloc[0]})\n",
    "            stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"Q2(50%)\", \"value\": q_values.iloc[1]})\n",
    "            stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"Q3(75%)\", \"value\": q_values.iloc[2]})\n",
    "        except: \n",
    "            stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"Q1(25%)\", \"value\": \"no value\"})\n",
    "            stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"Q2(50%)\", \"value\": \"no value\"})\n",
    "            stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"Q3(75%)\", \"value\": \"no value\"})\n",
    "        # format\n",
    "        try: \n",
    "            if not pd.isna(field_series['regex']): \n",
    "                regex_pattern = field_series['regex']\n",
    "                bool_series = field_series.str.match(regex_pattern, na=False)\n",
    "                stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"valid_format\", \"value\": bool_series.sum()})                \n",
    "            else: \n",
    "                stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"valid_format\", \"value\": \"err\"})\n",
    "        except: \n",
    "                stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"valid_format\", \"value\": \"no format\"})\n",
    "        # list\n",
    "        try: \n",
    "            if not pd.isna(field_series['list']): \n",
    "                values_lst = field_series['list']\n",
    "                bool_series = field_series.astype(str).isin(values_lst)\n",
    "                stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"valid_on_list\", \"value\": bool_series.sum()})                \n",
    "            else: \n",
    "                stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"valid_on_list\", \"value\": \"err\"})\n",
    "        except: \n",
    "                stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"valid_on_list\", \"value\": \"no list\"})\n",
    "        # range \n",
    "        try: \n",
    "            if not pd.isna(field_series['list']): \n",
    "                ranges_list = field_series['list']\n",
    "                range_lst = ranges_list.split(';') \n",
    "                min_limit, max_limit = range_lst[0], range_lst[1]\n",
    "                qty_range = ((field_series.astype(float) >= float(min_limit)) & (field_series.astype(float) <= float(max_limit))).sum()\n",
    "                stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"valid_on_range\", \"value\": qty_range})                \n",
    "            else: \n",
    "                stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"valid_on_range\", \"value\": \"err\"})\n",
    "        except: \n",
    "                stats_collected.append({\"table\": table_name, \"field\": field_name, \"stat\":\"valid_on_range\", \"value\": \"no range\"})\n",
    "    \n",
    "    # stats consolidation     \n",
    "    df_stats = pd.concat([df_stats,pd.DataFrame(stats_collected)])   \n",
    "df_stats = df_stats.astype('object')\n",
    "\n",
    "df_stats_pivot = (df_stats.pivot_table(index=[\"table\",\"field\"], columns=\"stat\", values=\"value\", fill_value=0,aggfunc='first').reset_index())\n",
    "df_stats_pivot_para_update = df_stats_pivot.set_index([\"table\", \"field\"])\n",
    "df_fields.update(df_stats_pivot_para_update)\n",
    "col_list = ['nulls', 'blanks', 'int', 'float', 'str', 'date']\n",
    "df_fields[col_list] = df_fields[col_list].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae384dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fld = df_fields.reset_index()\n",
    "col_series = df_fld.columns \n",
    "\n",
    "for idx_row, fld_row in df_fld.iterrows(): \n",
    "    line = int(idx_row) + 3\n",
    "    for col_name in col_series: \n",
    "        col = col_series.get_loc(col_name) + 1\n",
    "        #print(fld_row['table'], fld_row['field'], fld_row[col_name], col_name, line, col) \n",
    "        if col_name == \"size\": \n",
    "            fields_sheet.cell(row=line, column=col).value = \"Sergio\"\n",
    "        fields_sheet.cell(row=line, column=col).value = fld_row[col_name]\n",
    "        #print(fields_sheet.cell(row=line, column=col).value)\n",
    "\n",
    "wb.save(cfg.eda_sheet_full_path)\n",
    "\n",
    "        #fields_sheet.cell(row=line, column=col).value = str(stat_value) \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for idx, fld_row in df_fields.reset_index().iterrows(): \n",
    "#     field_name = fld_row['field']\n",
    "#     line = int(idx) + 2\n",
    "#     for col_name in df_fields.reset_index().columns.names:\n",
    "#         col = df_fields.reset_index().columns.get_loc(col_name) + 1 \n",
    "#         print(col_name,line, col)\n",
    "\n",
    "\n",
    "\n",
    "    # col_series = \n",
    "    # fields_sheet.cell(row=line, column=col).value = str(stat_value) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b6adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # col_type = df_types.apply(lambda col: col.value_counts().idxmax())\n",
    "    # for coluna, tipo in col_type.items():\n",
    "        \n",
    "    #     if tipo == \"int\":\n",
    "    #         df_dados[coluna] = pd.to_numeric(df_dados[coluna], errors=\"coerce\").astype(\"Int64\")\n",
    "    #     elif tipo == \"float\":\n",
    "    #         df_dados[coluna] = pd.to_numeric(df_dados[coluna], errors=\"coerce\").astype(\"Float64\")\n",
    "    #     elif tipo == \"date\":\n",
    "    #         df_dados[coluna] = pd.to_datetime(df_dados[coluna], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "    #     elif tipo in [\"null\", \"blank\"]:\n",
    "    #         df_dados[coluna] = pd.NA\n",
    "    #     else:\n",
    "    #         df_dados[coluna] = df_dados[coluna].astype(\"string\")\n",
    "\n",
    "\n",
    "\n",
    "    # print(repr(df_dados.iloc[1, 0]))\n",
    "    # print(type(df_dados.iloc[1, 0]))\n",
    "\n",
    "    # for coluna in df_dados.columns:\n",
    "    #     print(f'------ {coluna} ------') \n",
    "    #     qtd_nulls, qtd_blank, qtd_int, qtd_float, qtd_str, qtd_date = 0, 0, 0, 0, 0, 0\n",
    "    #     for valor in df_dados[coluna]:\n",
    "    #         type_class = classify_content(valor)\n",
    "\n",
    " \n",
    "    # with open(data_path, \"r\", newline='', encoding=encode(data_path)) as f:\n",
    "    #     reader = csv.reader(f)\n",
    "    #     # row_count = sum(1 for _ in reader)\n",
    "    #     # print(row_count)\n",
    "    #     for row_index, row in enumerate(reader):\n",
    "    #         for field_index, field_value in row:\n",
    "    #                 print(f\"Campo {field_index}: {field_value}\")\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "    # with open(data_path, \"r\", newline='', encoding=encode(data_path)) as f:\n",
    "    #     leitor = csv.DictReader(f, delimiter=\";\")\n",
    "    #     leitor.fieldnames = [nome.lower() for nome in leitor.fieldnames]\n",
    "    #     for coluna in df_dados.columns:\n",
    "    #         for linha in leitor:\n",
    "    #             print(linha[coluna])\n",
    "    #             print(type(linha[coluna]))\n",
    "    #             # print(linha)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # print(df_dados.columns)\n",
    "#     # print(df_dados.describe(include='all')) \n",
    "#     # print(df_dados[['noenti', 'cogrupo']].dtypes)\n",
    "\n",
    "#     # describe_collect     \n",
    "#     df_describe = df_dados.describe(include='all'); \n",
    "#     df_describe.columns = df_describe.columns.str.strip().str.lower()\n",
    "\n",
    "#     # print(df_describe.columns)\n",
    "\n",
    "#     # for id, reg in df_describe.iterrows(): \n",
    "#     #     print(reg)\n",
    "\n",
    "#     for idx, desc_row in df_describe.iterrows():\n",
    "#         stat_name = desc_row.name \n",
    "#         type_name = desc_row.dtype\n",
    "#         for col_name, value  in desc_row.items():\n",
    "#             field_name = col_name\n",
    "#             stat_value = value\n",
    "\n",
    "#             stats_collected.append({'table': table_name , 'field': field_name , 'stat': stat_name, 'value': stat_value})\n",
    "#         # dtype_field = df_dados[field_name].dtype\n",
    "#         # stats_collected.append({'table': table_name , 'field': field_name , 'stat': 'type', 'value': dtype_field})\n",
    "\n",
    "# df_stats = pd.concat([df_stats, pd.DataFrame(stats_collected)], ignore_index=True)\n",
    "# df_stats.rename(columns={'count': 'not-null'}, inplace=True)\n",
    "\n",
    "\n",
    "# for idx, reg in df_stats.iterrows(): \n",
    "#     print(reg['table'], reg['field'], reg['stat'], reg['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a60986f",
   "metadata": {},
   "source": [
    "Save stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a606460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tables sheet \n",
    "# header_tables = headers = {str(cell.value).lower(): idx+1 for idx, cell in enumerate(tables_sheet[1]) if cell.value}\n",
    "# # tables sheet loop\n",
    "# for tb_row in tables_sheet.iter_rows(min_row=2):\n",
    "#     print('===== Tables Loop =====')\n",
    "#     table_name = tb_row[header_tables['table']-1].value\n",
    "#     print(f\"=== {table_name} ===\")\n",
    "#     # fields sheet loop\n",
    "#     print('****** fields sheet loop ********')\n",
    "#     df_table_fields = df_fields[df_fields['table'] == table_name]\n",
    "#     for index, fld_row in df_table_fields.iterrows(): \n",
    "#         field_name = fld_row['field']\n",
    "#         print(f'*** {field_name} ***')\n",
    "#         line = int(index) + 2\n",
    "#         # field stats loop \n",
    "#         df_fld_stats = df_stats[df_stats['field'] == field_name]\n",
    "#         print('###### stats loop #####')\n",
    "#         for idx, stat_row in df_fld_stats.iterrows(): \n",
    "#             stat_name = stat_row['stat'] \n",
    "#             print(f' ## {stat_name} ##')\n",
    "#             col = df_fields.columns.get_loc(stat_name) + 1 \n",
    "#             try:\n",
    "#                 stat_value = str(stat_row['value']) \n",
    "#                 print(\"line/col: \",line, \" / \", col)\n",
    "#                 print(f'stat_value: {stat_value}')\n",
    "#                 fields_sheet.cell(row=line, column=col).value = str(stat_value) \n",
    "#             except: \n",
    "#                 print('exception')\n",
    "#                 continue\n",
    "#     print(\"\")\n",
    "#     print(\"-*\" * 50 )\n",
    "#     print(\"\")\n",
    "\n",
    "# wb.save(cfg.eda_sheet_full_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda-to-exel-rIqPlTpX-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
